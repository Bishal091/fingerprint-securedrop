; This file provides a centralized configuration mechanism for those wishing to
; use sorter.py and crawler.py without tapping into the API. The configuration
; options in this file are still intended to be powerful enough to allow one to
; conduct website fingerprinting research on a completely different site or set
; of sites than SecureDrop.

[sorter]
; Comma-delimited list of hidden service directories
onion_dirs = http://secrdrop5wyphb5x.onion/sites/securedrop.org/files/securedrop_list.txt
  http://msydqstlz2kzerdg.onion/address/
  http://skunksworkedp2cg.onion/sites.html
; Time to wait for a onion service to load before timing out
page_load_timeout = 20
; Number of asynchronous connections to have open at a time. The higher this
; value is, the faster the sorter works, but if set too high, connection errors
; may increase significantly.
max_tasks = 10
; The Sorter sorts sites based on user-defined boolean expressions that operate
; on the HTML body of pages. The key is the class name and the value is some
; boolean expression that operates on the HTML body object reference `text`.
; Order matters: a site will be sorted into the first class for which the
; boolean expression evaluates to true.
class_tests = {"sd_0310": "'Powered by SecureDrop 0.3.10.' in text"}
  {"nonmonitored": "'SecureDrop' not in text"}
; Do you want this data to be inserted into the database?
use_database = true

[crawler]
; Whether the crawler should use the database
use_database = false
; How far back in our sorting history should the crawler look,
; accepts integer and single character unit: '4w', '1d', '1h'
; This only works if you use the database (e.g., if we scraped/"sorted" every
; day last week, we might want to try a site that was up on W but not Th).
hs_history_lookback = 1w
; If use_database is set to false then the crawler will use a pickle file
class_data = class-data_07-28_18:15:40.pickle
; Time to wait for a onion service to load before timing out
page_load_timeout = 20
; Time to wait after a page has succesfully loaded to catch traffic after
; initial onload event
wait_on_page = 5
; Time to wait between closing all open circuits and starting collection of the
; next trace
wait_after_closing_circuits = 5
; Experimental: whether or not to restart tor in the case that an exception
; known to crash the Crawler is encountered. Still not confirmed if this
; prevents Crawler crash.
restart_on_sketchy_exception = True
; Traces to record of each monitored site for every trace of a non-monitored
; site
monitored_nonmonitored_ratio = 10
; List of preferred EntryNodes. Tor will make the first
; sequentially listed one reachable the client guard.
entry_nodes = 237C733E2D1BAD26FAABEAFB612BDA0CD3BC6AF4,95CDFAA852FF075FCD23102C06636F2B84B3371A,E727D0B4179549BB3F82ED3C256F209E54CE0B23,41E2FB97690EED79DD4BD6BAC00A974B69F49858

[database]
; The username to authenticate to your production database. Password should be
; in a PGPASSFILE (normally generated for you by Ansible).
pguser = fp_user
; The IP of your database
pghost = localhost
; The port on which to connect to your production database>
pgport = 5432
; Your production database name.
pgdatabase = fpsd

[test_database]
; Only necessary if you wish to run the tests.
pguser = fp_user
pghost = localhost
pgport = 5432
pgdatabase = testfpsd
